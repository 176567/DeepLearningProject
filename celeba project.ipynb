{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Download dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "made with Python 3.10.4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "ESvMjnVl80Vg"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow_datasets as tfds\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import csv"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Couldn't download the data with code, because it timed out. So we load the downloaded data in with the image_dataset_from_directory function"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "creating a new file (data.csv), with better format to read in with a generator, values are separated by coma and each line contains the file name, 1 or 0 for each attribute, "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "#only need to run once, than use data.csv file to load labels\n",
        "Y = []\n",
        "file_path = 'data.csv'\n",
        "\n",
        "with open(\"list_attr_celeba.txt\") as attr:\n",
        "    with open(file_path, 'w', newline='') as file:\n",
        "        writer = csv.writer(file)\n",
        "        count = 0\n",
        "        size = next(attr)\n",
        "        labels = next(attr).strip()\n",
        "        firstRow = [\"Filenames\"] + labels.split()\n",
        "        writer.writerow(firstRow)\n",
        "        for line in attr:\n",
        "            name = line[:11]\n",
        "            line = line[11:]\n",
        "            numbers = line.split()\n",
        "            for (i, number) in enumerate(numbers):\n",
        "                if number == \"-1\":\n",
        "                    numbers[i] = 0\n",
        "                elif number == \"1\":\n",
        "                    numbers[i] = 1\n",
        "            data = [name] + numbers\n",
        "            writer.writerow(data)\n",
        "            "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "with open(\"list_attr_celeba.txt\") as attr:\n",
        "        count = 0\n",
        "        size = next(attr)\n",
        "        labels = next(attr).strip()\n",
        "labels = labels.split()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "load data with generators"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "right now, only use part of the data, so it's easier to handle"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found 1800 non-validated image filenames.\n",
            "Found 100 non-validated image filenames.\n",
            "Found 100 non-validated image filenames.\n"
          ]
        }
      ],
      "source": [
        "df=pd.read_csv(\"./img_align_celeba/data.csv\")\n",
        "columns=labels\n",
        "datagen=ImageDataGenerator(rescale=1./255.)\n",
        "test_datagen=ImageDataGenerator(rescale=1./255.)\n",
        "train_generator=datagen.flow_from_dataframe(\n",
        "dataframe=df[:1800],\n",
        "directory=\"./img_align_celeba/img_align_celeba\",\n",
        "x_col=\"Filenames\",\n",
        "validate_filenames=False,\n",
        "y_col=columns,\n",
        "batch_size=32,\n",
        "seed=42,\n",
        "shuffle=True,\n",
        "class_mode=\"raw\",\n",
        "target_size=(178,218))\n",
        "\n",
        "valid_generator=test_datagen.flow_from_dataframe(\n",
        "dataframe=df[1800:1900],\n",
        "directory=\"./img_align_celeba/img_align_celeba\",\n",
        "x_col=\"Filenames\",\n",
        "validate_filenames=False,\n",
        "y_col=columns,\n",
        "batch_size=32,\n",
        "seed=42,\n",
        "shuffle=True,\n",
        "class_mode=\"raw\",\n",
        "target_size=(178,218))\n",
        "\n",
        "test_generator=test_datagen.flow_from_dataframe(\n",
        "dataframe=df[1900:2000],\n",
        "directory=\"./img_align_celeba/img_align_celeba\",\n",
        "x_col=\"Filenames\",\n",
        "validate_filenames=False,\n",
        "batch_size=1,\n",
        "seed=42,\n",
        "shuffle=False,\n",
        "class_mode=None,\n",
        "target_size=(178,218))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d (Conv2D)             (None, 176, 216, 32)      896       \n",
            "                                                                 \n",
            " max_pooling2d (MaxPooling2  (None, 88, 108, 32)       0         \n",
            " D)                                                              \n",
            "                                                                 \n",
            " conv2d_1 (Conv2D)           (None, 86, 106, 64)       18496     \n",
            "                                                                 \n",
            " max_pooling2d_1 (MaxPoolin  (None, 43, 53, 64)        0         \n",
            " g2D)                                                            \n",
            "                                                                 \n",
            " conv2d_2 (Conv2D)           (None, 41, 51, 128)       73856     \n",
            "                                                                 \n",
            " max_pooling2d_2 (MaxPoolin  (None, 20, 25, 128)       0         \n",
            " g2D)                                                            \n",
            "                                                                 \n",
            " flatten (Flatten)           (None, 64000)             0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 128)               8192128   \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 40)                5160      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 8290536 (31.63 MB)\n",
            "Trainable params: 8290536 (31.63 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "# Define the CNN model\n",
        "model = tf.keras.Sequential([\n",
        "    layers.Conv2D(32, (3, 3), activation='relu', input_shape=(178, 218, 3)),\n",
        "    layers.MaxPooling2D((2, 2)),\n",
        "    layers.Conv2D(64, (3, 3), activation='relu'),\n",
        "    layers.MaxPooling2D((2, 2)),\n",
        "    layers.Conv2D(128, (3, 3), activation='relu'),\n",
        "    layers.MaxPooling2D((2, 2)),\n",
        "    layers.Flatten(),\n",
        "    layers.Dense(128, activation='relu'),\n",
        "    layers.Dense(40, activation='sigmoid')  # Assuming there are 40 facial attributes to classify\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam',\n",
        "              loss='binary_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# Print the model summary\n",
        "model.summary()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\Zear\\AppData\\Local\\Temp\\ipykernel_2248\\3771039389.py:4: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
            "  model.fit_generator(generator=train_generator,\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "56/56 [==============================] - 72s 1s/step - loss: 0.4411 - accuracy: 0.0090 - val_loss: 0.4006 - val_accuracy: 0.0000e+00\n",
            "Epoch 2/10\n",
            "56/56 [==============================] - 61s 1s/step - loss: 0.3781 - accuracy: 0.0045 - val_loss: 0.3510 - val_accuracy: 0.0104\n",
            "Epoch 3/10\n",
            "56/56 [==============================] - 60s 1s/step - loss: 0.3201 - accuracy: 0.0130 - val_loss: 0.3286 - val_accuracy: 0.0104\n",
            "Epoch 4/10\n",
            "56/56 [==============================] - 65s 1s/step - loss: 0.2737 - accuracy: 0.0147 - val_loss: 0.3199 - val_accuracy: 0.0104\n",
            "Epoch 5/10\n",
            "56/56 [==============================] - 59s 1s/step - loss: 0.2337 - accuracy: 0.0164 - val_loss: 0.3514 - val_accuracy: 0.0104\n",
            "Epoch 6/10\n",
            "56/56 [==============================] - 48s 853ms/step - loss: 0.1890 - accuracy: 0.0192 - val_loss: 0.3731 - val_accuracy: 0.0208\n",
            "Epoch 7/10\n",
            "56/56 [==============================] - 47s 845ms/step - loss: 0.1417 - accuracy: 0.0175 - val_loss: 0.4342 - val_accuracy: 0.0208\n",
            "Epoch 8/10\n",
            "56/56 [==============================] - 48s 853ms/step - loss: 0.1013 - accuracy: 0.0249 - val_loss: 0.4750 - val_accuracy: 0.0104\n",
            "Epoch 9/10\n",
            "56/56 [==============================] - 50s 884ms/step - loss: 0.0715 - accuracy: 0.0243 - val_loss: 0.5786 - val_accuracy: 0.0104\n",
            "Epoch 10/10\n",
            "56/56 [==============================] - 51s 914ms/step - loss: 0.0552 - accuracy: 0.0215 - val_loss: 0.6161 - val_accuracy: 0.0000e+00\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x1bd213d2470>"
            ]
          },
          "execution_count": 39,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#todo change from model.fit_generator to model.fit because it is deprecated\n",
        "STEP_SIZE_TRAIN=train_generator.n//train_generator.batch_size\n",
        "STEP_SIZE_VALID=valid_generator.n//valid_generator.batch_size\n",
        "STEP_SIZE_TEST=test_generator.n//test_generator.batch_size\n",
        "model.fit_generator(generator=train_generator,\n",
        "                    steps_per_epoch=STEP_SIZE_TRAIN,\n",
        "                    validation_data=valid_generator,\n",
        "                    validation_steps=STEP_SIZE_VALID,\n",
        "                    epochs=10\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
