{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "made with Python 3.10.4\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Download dataset img_align_celeba.zip from https://drive.google.com/drive/folders/0B7EVK8r0v71pTUZsaXdaSnZBZzg?resourcekey=0-rJlzl934LzC-Xp28GeIBzQ\n",
        "unzip it and put it in the ./img_align_celba/img_align_celeba folder"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "install all packeges listed in requirements.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 142,
      "metadata": {
        "id": "ESvMjnVl80Vg"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import csv\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.initializers import HeNormal\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from datetime import datetime\n",
        "from tensorflow.keras.metrics import BinaryAccuracy, Precision, Recall, AUC, F1Score"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "creating a new file (data.csv), with better format to read in with a generator, values are separated by coma and each line contains the file name, 1 or 0 for each attribute, "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "list_attr_celeba.txt was provided on the website along with the images"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 143,
      "metadata": {},
      "outputs": [],
      "source": [
        "#only need to run once, than use data.csv file to load labels\n",
        "file_path = 'data.csv'\n",
        "\n",
        "with open(\"list_attr_celeba.txt\") as attr:\n",
        "    with open(file_path, 'w', newline='') as file:\n",
        "        writer = csv.writer(file)\n",
        "        count = 0\n",
        "        size = next(attr)\n",
        "        labels = next(attr).strip()\n",
        "        firstRow = [\"Filenames\"] + labels.split()\n",
        "        writer.writerow(firstRow)\n",
        "        for line in attr:\n",
        "            name = line[:11]\n",
        "            line = line[11:]\n",
        "            numbers = line.split()\n",
        "            for (i, number) in enumerate(numbers):\n",
        "                if number == \"-1\":\n",
        "                    numbers[i] = 0\n",
        "                elif number == \"1\":\n",
        "                    numbers[i] = 1\n",
        "            data = [name] + numbers\n",
        "            writer.writerow(data)\n",
        "            "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This has to be run every time, it gets the label names and the number off attributes from the labels file"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 144,
      "metadata": {},
      "outputs": [],
      "source": [
        "with open(\"list_attr_celeba.txt\") as attr:\n",
        "        count = 0\n",
        "        size = next(attr)\n",
        "        labels = next(attr).strip()\n",
        "labels = labels.split()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 145,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "202599 162079 182339\n"
          ]
        }
      ],
      "source": [
        "#calculate the split so that we have 80% training, 10% validation, 10% testing\n",
        "total = int(size)\n",
        "train_val_split_idx = int(total * 0.8)\n",
        "val_test_split_idx = int(total * 0.9)\n",
        "print(total, train_val_split_idx, val_test_split_idx)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We are using generators, to load the data, preprocess it, rescale... and generate more images with horizontal flipping and zooming. The files are loaded from the "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 146,
      "metadata": {},
      "outputs": [
        {
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: './img_align_celeba/data.csv'",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[146], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m df\u001b[38;5;241m=\u001b[39m\u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m./img_align_celeba/data.csv\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      2\u001b[0m columns\u001b[38;5;241m=\u001b[39mlabels\n\u001b[0;32m      3\u001b[0m horizontal_flip \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
            "File \u001b[1;32mc:\\Users\\Zear\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:1026\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[0;32m   1013\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[0;32m   1014\u001b[0m     dialect,\n\u001b[0;32m   1015\u001b[0m     delimiter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1022\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[0;32m   1023\u001b[0m )\n\u001b[0;32m   1024\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m-> 1026\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mc:\\Users\\Zear\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:620\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    617\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[0;32m    619\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[1;32m--> 620\u001b[0m parser \u001b[38;5;241m=\u001b[39m TextFileReader(filepath_or_buffer, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m    622\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[0;32m    623\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
            "File \u001b[1;32mc:\\Users\\Zear\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:1620\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m   1617\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m   1619\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1620\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mc:\\Users\\Zear\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:1880\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[1;34m(self, f, engine)\u001b[0m\n\u001b[0;32m   1878\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[0;32m   1879\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m-> 1880\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1881\u001b[0m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1882\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1883\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1884\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcompression\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1885\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmemory_map\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1886\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1887\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding_errors\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstrict\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1888\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstorage_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1889\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1890\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1891\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
            "File \u001b[1;32mc:\\Users\\Zear\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\io\\common.py:873\u001b[0m, in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    868\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m    869\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[0;32m    870\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[0;32m    871\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[0;32m    872\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[1;32m--> 873\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[0;32m    874\u001b[0m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    875\u001b[0m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    876\u001b[0m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    877\u001b[0m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    878\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    879\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    880\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    881\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[0;32m    882\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
            "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './img_align_celeba/data.csv'"
          ]
        }
      ],
      "source": [
        "df=pd.read_csv(\"./img_align_celeba/data.csv\")\n",
        "columns=labels\n",
        "horizontal_flip = True\n",
        "zoom_range = 0.2\n",
        "datagen=ImageDataGenerator(rescale=1./255.)\n",
        "test_datagen=ImageDataGenerator(rescale=1./255.)\n",
        "train_generator=datagen.flow_from_dataframe(\n",
        "dataframe=df[:train_val_split_idx],\n",
        "directory=\"./img_align_celeba/img_align_celeba\", #path to images\n",
        "x_col=\"Filenames\",  #column in csv file that has the filenames\n",
        "validate_filenames=False, \n",
        "y_col=columns, #columns in csv file that has the labels\n",
        "batch_size=32, \n",
        "seed=42, #we are setting the seed, so that we can get the same results each time we run the code, don't have multiple variables\n",
        "shuffle=True,\n",
        "class_mode=\"raw\", #we already have the labels in the appropriate format in the file\n",
        "target_size=(178,218) #all images are already this size, but just in case...\n",
        ")\n",
        "\n",
        "valid_generator=test_datagen.flow_from_dataframe(\n",
        "dataframe=df[train_val_split_idx:val_test_split_idx],\n",
        "directory=\"./img_align_celeba/img_align_celeba\",\n",
        "x_col=\"Filenames\",\n",
        "validate_filenames=False,\n",
        "y_col=columns,\n",
        "batch_size=32,\n",
        "seed=42,\n",
        "shuffle=True,\n",
        "class_mode=\"raw\", #we already have the labels in the appropriate format\n",
        "target_size=(178,218))\n",
        "\n",
        "test_generator=test_datagen.flow_from_dataframe(\n",
        "dataframe=df[val_test_split_idx:],\n",
        "directory=\"./img_align_celeba/img_align_celeba\",\n",
        "x_col=\"Filenames\",\n",
        "validate_filenames=False,\n",
        "batch_size=32,\n",
        "y_col = columns,\n",
        "seed=42,\n",
        "shuffle=False,\n",
        "class_mode=\"raw\",\n",
        "target_size=(178,218))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 147,
      "metadata": {},
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "# Define the CNN model\n",
        "model = tf.keras.Sequential([\n",
        "    layers.Conv2D(32, (3, 3), activation='relu', kernel_initializer = HeNormal() ,input_shape=(178, 218, 3)),\n",
        "    layers.MaxPooling2D((2, 2)),\n",
        "    layers.Conv2D(64, (3, 3), activation='relu', kernel_initializer = HeNormal()),\n",
        "    layers.MaxPooling2D((2, 2)),\n",
        "    layers.Conv2D(128, (3, 3), activation='relu', kernel_initializer = HeNormal()),\n",
        "    layers.MaxPooling2D((2, 2)),\n",
        "    layers.Flatten(),\n",
        "    layers.Dense(128, activation='relu', kernel_initializer = HeNormal()),\n",
        "    layers.Dense(40, activation='sigmoid')  # There are 40 facial attributes to classify\n",
        "])\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 148,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define the early stopping callback\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 149,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Compile the model\n",
        "model.compile(optimizer='adam',\n",
        "              loss='binary_crossentropy',\n",
        "              metrics=[BinaryAccuracy(name='accuracy'),Precision(name='precision'), Recall(name='recall'), AUC(name='auc')])\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 150,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"sequential_4\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d_16 (Conv2D)          (None, 176, 216, 32)      896       \n",
            "                                                                 \n",
            " max_pooling2d_16 (MaxPooli  (None, 88, 108, 32)       0         \n",
            " ng2D)                                                           \n",
            "                                                                 \n",
            " conv2d_17 (Conv2D)          (None, 86, 106, 64)       18496     \n",
            "                                                                 \n",
            " max_pooling2d_17 (MaxPooli  (None, 43, 53, 64)        0         \n",
            " ng2D)                                                           \n",
            "                                                                 \n",
            " conv2d_18 (Conv2D)          (None, 41, 51, 128)       73856     \n",
            "                                                                 \n",
            " max_pooling2d_18 (MaxPooli  (None, 20, 25, 128)       0         \n",
            " ng2D)                                                           \n",
            "                                                                 \n",
            " flatten_5 (Flatten)         (None, 64000)             0         \n",
            "                                                                 \n",
            " dense_8 (Dense)             (None, 128)               8192128   \n",
            "                                                                 \n",
            " dense_9 (Dense)             (None, 40)                5160      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 8290536 (31.63 MB)\n",
            "Trainable params: 8290536 (31.63 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "# Print the model summary\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 151,
      "metadata": {},
      "outputs": [],
      "source": [
        "#define the steps per epoch for training, validation and testing \n",
        "STEP_SIZE_TRAIN = train_generator.n // train_generator.batch_size\n",
        "STEP_SIZE_VALID = valid_generator.n // valid_generator.batch_size\n",
        "STEP_SIZE_TEST = test_generator.n // test_generator.batch_size"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "If step size is too big, (too long training time, we can decrease it temporarily). \n",
        "\n",
        "Number of epochs should be set according to the training length, preferebly it should be \"infinity\" so that the training is only stopped due to early stopping."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 152,
      "metadata": {},
      "outputs": [],
      "source": [
        "#lower step sizes for quicker training, when testing different models, hyperparameters\n",
        "STEP_SIZE_TRAIN = 100\n",
        "STEP_SIZE_VALID = 10"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 153,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "100/100 [==============================] - 116s 1s/step - loss: 0.4976 - accuracy: 0.8195 - precision: 0.6538 - recall: 0.4101 - auc: 0.7946 - val_loss: 0.3763 - val_accuracy: 0.8450 - val_precision: 0.7286 - val_recall: 0.4648 - val_auc: 0.8435\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x1dc84526dd0>"
            ]
          },
          "execution_count": 153,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model.fit(\n",
        "    train_generator,\n",
        "    steps_per_epoch=STEP_SIZE_TRAIN,\n",
        "    validation_data=valid_generator,\n",
        "    validation_steps=STEP_SIZE_VALID,\n",
        "    epochs=1,\n",
        "    callbacks=[early_stopping]\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 154,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Save the model (putting timestamp so we can keep track of different models)\n",
        "t = datetime.now()\n",
        "current_time = t.strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
        "model.save(f'./model{current_time}.keras')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:Skipping variable loading for optimizer 'RMSprop', because it has 11 variables whereas the saved optimizer has 1 variables. \n"
          ]
        }
      ],
      "source": [
        "#file name needs to be modified to load the model\n",
        "model = tf.keras.models.load_model('./model.keras')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 156,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "100/100 [==============================] - 34s 337ms/step - loss: 0.3770 - accuracy: 0.8423 - precision: 0.7561 - recall: 0.4781 - auc: 0.8534\n"
          ]
        }
      ],
      "source": [
        "# Evaluate the model on the test data\n",
        "test_eval = model.evaluate(test_generator, steps=STEP_SIZE_TEST)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 157,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test loss      : 0.3770066499710083\n",
            "Test accuracy  : 84.230%\n",
            "Test precision : 75.612%\n",
            "Test recall    : 47.811%\n",
            "Test AUC       : 85.339%\n"
          ]
        }
      ],
      "source": [
        "print('Test loss      : {}'.format(test_eval[0]))\n",
        "print('Test accuracy  : {:.3%}'.format(test_eval[1]))\n",
        "print('Test precision : {:.3%}'.format(test_eval[2]))\n",
        "print('Test recall    : {:.3%}'.format(test_eval[3]))\n",
        "print('Test AUC       : {:.3%}'.format(test_eval[4]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
